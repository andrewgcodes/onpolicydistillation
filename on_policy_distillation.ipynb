{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this Colab notebook, you'll do on-policy distillation (OPD) on Qwen3-0.6b using Qwen3-4b-Instruct-2507, to make it better at [GSM8K](https://huggingface.co/datasets/openai/gsm8k) (a dataset of math problems).\n",
    "\n",
    "Unlike standard supervised fine-tuning (SFT), the student model (Qwen-3-0.6b) learns from its own generated outputs rather than fixed gold data — reducing exposure bias and better matching the inference-time distribution.\n",
    "\n",
    "You'll need to connect an A100 GPU (40 GB Ram) or better. You might be able to get away with smaller GPUs if you change some of the config parameters, like samples_per_prompt and max_new_tokens!"
   ],
   "metadata": {
    "id": "0O3kjUDwUKxW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspired by [Thinking Machines](https://thinkingmachines.ai/blog/on-policy-distillation/) and prior art like [Agarwal et al](https://arxiv.org/abs/2306.13649)."
   ],
   "metadata": {
    "id": "O7PRWDrLT8BM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzHpn5I0DoyP",
    "outputId": "9621c1ef-241a-4393-812c-cb369db20c9a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-84373428-e84d-9a3b-e6a4-2771d4c1a0d0)\n",
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mTransformers: 4.51.3\n",
      "Accelerate: 1.4.0\n",
      "PEFT: 0.14.0\n",
      "matplotlib: 3.10.0\n",
      "== Environment ==\n",
      "{'python': '3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]', 'torch': '2.8.0+cu126', 'transformers': '4.51.3', 'accelerate': '1.4.0', 'peft': '0.14.0', 'cuda': '12.6', 'device_name': 'NVIDIA A100-SXM4-40GB', 'platform': 'Linux-6.6.105+-x86_64-with-glibc2.35', 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "#@title 🛠️ Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform, math\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Qwen3 requires Transformers >= 4.51\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (A100+ recommended).\"\n",
    "\n",
    "def print_header():\n",
    "    print(\"== Environment ==\")\n",
    "    print(dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        transformers=transformers.__version__,\n",
    "        accelerate=accelerate.__version__,\n",
    "        peft=peft.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        platform=platform.platform(),\n",
    "        seed=SEED\n",
    "    ))\n",
    "print_header()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, time, json, random, platform\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import re\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility & device\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"A CUDA GPU is required.\"\n"
   ],
   "metadata": {
    "id": "x_5x6fL3Ut0X"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try toying around with these settings. You can also try using a different teacher model, even if it doesn't use the same tokenizer."
   ],
   "metadata": {
    "id": "v1ttYTs9Uv29"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Models\n",
    "    student_id: str = \"Qwen/Qwen3-0.6B-Base\"\n",
    "    teacher_id: str = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "    # Prompting\n",
    "    prompt_template: str = (\n",
    "        \"Solve step by step.\\n\"\n",
    "        \"Give ONLY ONE final numeric answer (no units), inside square brackets.\\n\"\n",
    "        \"Problem: {question}\\n\\nSolution:\"\n",
    "    )\n",
    "    max_new_tokens: int = 256\n",
    "\n",
    "    # Generation temps\n",
    "    eval_temperature: float = 0.0   # greedy for eval\n",
    "    train_temperature: float = 0.7  # sampling for on-policy data\n",
    "\n",
    "    # Training schedule\n",
    "    steps: int = 50\n",
    "    batch_prompts: int = 4\n",
    "    samples_per_prompt: int = 4 # increase this if you have a H200/B200\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "    grad_accum: int = 1\n",
    "\n",
    "    # Micro-batching\n",
    "    student_mb: int = 8\n",
    "\n",
    "    # Monitoring\n",
    "    log_every: int = 10\n",
    "    val_every: int = 10\n",
    "    val_sample_n: int = 100\n",
    "    ema_momentum: float = 0.9\n",
    "\n",
    "    # Validation size\n",
    "    val_rows: Optional[int] = None  # if None, uses min(200, len(train))\n",
    "\n",
    "    # Output dir\n",
    "    run_root: str = f\"./run_opd_{int(time.time())}\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.run_root, exist_ok=True)\n",
    "\n",
    "def print_env():\n",
    "    import transformers, accelerate, peft, matplotlib\n",
    "    print(\"== Environment ==\")\n",
    "    print({\n",
    "        \"python\": sys.version,\n",
    "        \"torch\": torch.__version__,\n",
    "        \"transformers\": transformers.__version__,\n",
    "        \"accelerate\": accelerate.__version__,\n",
    "        \"peft\": peft.__version__,\n",
    "        \"cuda\": torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        \"platform\": platform.platform(),\n",
    "        \"seed\": SEED\n",
    "    })\n",
    "print_env()\n",
    "\n",
    "# --------------------------\n",
    "# Data: GSM8K\n",
    "# --------------------------\n",
    "def render_prompt(question: str) -> str:\n",
    "    return cfg.prompt_template.format(question=question)\n",
    "\n",
    "def parse_gold(answer_text: str) -> Optional[str]:\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def parse_pred(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\]\", text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "print(\"Loading GSM8K…\")\n",
    "ds_train_full = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "ds_test       = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "if cfg.val_rows is None:\n",
    "    val_rows = min(200, len(ds_train_full))\n",
    "else:\n",
    "    val_rows = min(cfg.val_rows, len(ds_train_full))\n",
    "\n",
    "ds_val   = ds_train_full.select(range(val_rows))\n",
    "ds_train = ds_train_full.select(range(val_rows, len(ds_train_full)))\n",
    "\n",
    "print(f\"Splits: {len(ds_train)} train | {len(ds_val)} val | {len(ds_test)} test\")\n",
    "\n",
    "# --------------------------\n",
    "# Tokenizers & Models\n",
    "# --------------------------\n",
    "def load_tokenizers(student_id: str, teacher_id: str):\n",
    "    tok_s = AutoTokenizer.from_pretrained(student_id, use_fast=True)\n",
    "    tok_t = AutoTokenizer.from_pretrained(teacher_id, use_fast=True)\n",
    "    for tok in (tok_s, tok_t):\n",
    "        if tok.pad_token is None and tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        tok.padding_side = \"left\"  # decoder-only: left padding\n",
    "    return tok_s, tok_t\n",
    "\n",
    "def make_lora_student(model_id: str) -> torch.nn.Module:\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    base.config.use_cache = False  # off for training\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "def load_teacher(model_id: str) -> torch.nn.Module:\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    ).eval()\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return m\n",
    "\n",
    "tok_s, tok_t = load_tokenizers(cfg.student_id, cfg.teacher_id)\n",
    "print(\"Padding sides:\", tok_s.padding_side, tok_t.padding_side)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649,
     "referenced_widgets": [
      "828b3bb4d2df4f5c8332bb9c44f561e0",
      "4dfa8b94d60c4496b9fbebd069d39475",
      "cb8b3b2b3a6b4719a80b33c18b685f57",
      "4156f40d18174fd4b051e4e32baf3ca2",
      "cfbd388fabe84093b7f374faff98ba18",
      "de577ae09448450eb1fb05531635081a",
      "9183266e2e324ff48dfb2e532dec11c0",
      "a70223dffdb54c8d8a7a582f614aa101",
      "974d350d09d94fcabb2f461b1272e420",
      "663adf401ada42c382f72c40993d0fba",
      "9cb1f562cb0949ff9b95ca2889a3d020",
      "c14c2eac7d434ffd896e33e3e9bfdc98",
      "d6d2c6e8762f4c6b8dcb51135698512c",
      "80fb6b84377d4875b731db072efe2d3b",
      "16c15a3f10f24b58993f650bbad680cb",
      "cdce465d5040417487caa1c92f514ee4",
      "72453b680ab1435ca3202b7ab3add760",
      "df6506cb7c7344eba9ded9f968740256",
      "3cb56d21813d423080ce0c029df3540e",
      "fe9f2cc368924c518157caee1dcd2dbb",
      "8e6a3f7890094044ac9160b143598584",
      "69f27cb714b34674ae0429abbfd5a8ce",
      "8551488212854178b74385b209b47bf8",
      "766dbcd90d3c4df18d1888edc18cfbf7",
      "3ab1897b9d2047898e3462be07c8ad0a",
      "b528bf6301104662986c6e08cf3e18c0",
      "a1742babdb9f4cab9007835d826a3f31",
      "30f31854469546918a0958677636bf93",
      "80e33a6d33b7454ab5dcbec5bb4c61b8",
      "1b89848e0be04952b77937822a0b02c9",
      "c475518de76a48a1873d6f092a3c9d91",
      "c9a9aa005bb4465ea1af82fcd9d8afd4",
      "7a19b627909d4203b6c50e541c3b3bf3",
      "474533c58b8242d898b2fac4d8869c52",
      "a09ed2a071c9415dbc6db02d46cf4844",
      "1fabddef86694495b2f40a7d31e1efe1",
      "f99b66bc4b084a10a6b2739254bd11cd",
      "92aead18ca9c42fcb5a8126c425a367c",
      "4598b67d1ab34fac97301e37e36e807e",
      "a49fbb6e19d64101a7425006540d5b81",
      "0580907c64fd4d7e8629f48fe2206b7e",
      "1356e073513d40dc986286fb68dc5cde",
      "40ed823de9654f9396c67a75094c6533",
      "478bc78af9ec4b438f99e750db8d2e9a",
      "b79a82873e1548c9b251ab67e7c84de0",
      "b3a36f3d988e4c1f9df346c19efe1960",
      "73f222c0e66a4578b26b28d7603e4617",
      "0fe4f10c1f4a4152a5beb0fbc7fdaa32",
      "354062c80b5d481a813d4befbec2c4d7",
      "66b69ae7def9456a960cb9805c998bb5",
      "6853185373e94a5f96d068d7cb8bdd5f",
      "f9df6d1ea08348d1babd0331dde54419",
      "a3276057dd994c5fbbedcc60a5e9178f",
      "7a6ad553d9854601b6c320f8a95ffc7b",
      "d5087c5ce3334860b747481dd6f2cf7a",
      "dbe521d6446d4cea8cc7af8c702b28c2",
      "63c15f6d94c34468a10cbaaf99307364",
      "9f32cba38ab847a1aa8bc2882a53c1f8",
      "c7071ad65a61477288c42a7fc4fde530",
      "53a1c9cf15ba4dfa8ac9cb4b3b93442a",
      "0b4986bc3b694b5484dcfbbef5e41179",
      "95745730b2314b08b85d83340d961321",
      "a1152b6dc04342b2bc98d670eaf8b9da",
      "f08821d894fa4c06a83eb8d9351f3e75",
      "181662afc9d141f099840fa0fa283423",
      "202b9c3043b44c428bcfaf0da6dda6cc",
      "fdc72c477027405d850c978ef0fbeeaf",
      "5a31fc5df00d449fa50bbfd8a3f1cd50",
      "8fb64fe4bb9c4364a1ea2e34821fdbfb",
      "b418a33e98c542ae8f29f667f3f13ece",
      "b33234068ceb4cb68bf4de79b18c3ac8",
      "29b7ce0e3225450ea4a7031d6be205e8",
      "63b01dbfc5214a42b66a9154d99c515c",
      "6945be98814b49f3881201714d46cf46",
      "309f09862d98463a87e02b1fb9fd05bb",
      "d89cc38eee614b0ab11789d02d76a4d5",
      "b9a3fb7de71040d487410695cb6df583",
      "b9cf70e54cb746bf9c49cfbc61dd5a1b",
      "bff672c5059a4556a7371d478b15fdd2",
      "cc551aa4588b4f4ca2db8be3be14967a",
      "48a91e735230440398740a90d10dd1c4",
      "39581f905d21436187704f1d5274284e",
      "8be8620f01af4fcc85107295cf7afb98",
      "302b00a7b59f413ebf277c12bbad51c3",
      "323d25f425e14b95961b8aac7b217361",
      "fe41146454ba468099fbf4078e7569a2",
      "b9490ccd44144dcc83775ea3f51698e1",
      "6f189547158741b5b12d53f6a48ef9af",
      "f7e25a4b3caf4d1494a1931b2b8a4c4f",
      "7cc2cfca1167456fb4d70cbc820550e6",
      "5baf2b2da1714007be45a58e020d9bfb",
      "395b1475cdce4efb9838e59d062331f2",
      "113cec096d7441ec8b32cd994fe6cb59",
      "9a4e4126fbcc4e1bbae1d2a1914771ca",
      "02633749512a4268808a04005e948c98",
      "7e4c5f209cb3419ca5eb8af7e719ed52",
      "4963343b137d447c8959e9f4f8c72864",
      "58462486ba634d249c8c2dc70b3f4375",
      "1f80a75b69c14ad58d5c87e0d8c952a3",
      "96faaf1fd485441ab9184063f5a3266b",
      "b498d18a3e4f497383298dff00aaada9",
      "94214ef00121486c89e7fded87488625",
      "52f1407c9dc6412cb9d32e1ab096caaf",
      "3131bfe6bcb145149d9c31387f1eaf25",
      "48eec6dad965467ca5e011fbf0952f5e",
      "5007f440b7704dbd87c236afdf53f5ae",
      "4ae9d31c8248443a9b332658503bb107",
      "aeea53225326484a9d1cc4ebcf555623",
      "c0663fa90370493c98d387e02fe4aada",
      "5b9dd6b0b3a743d28aea0d8e7e153642",
      "24eca0174df04bd89f8db4b2712ffe95",
      "42a351903d604cc3afde43b28fe6a9f4",
      "37a6720a163f4d0eb2384b9334385276",
      "6b2b0e2981474fc385cbaebfd492712b",
      "7bcae34b8d0b4691b274caa22402bb9c",
      "d0011c79284b419ea02c328ce1d5ca58",
      "a1f68f0132f5434da6797f4dc70da8b1",
      "cc31e592f52d4feabb571b7067112ae1",
      "4b8d8e32a59048b8a0b6d10cd47592d6",
      "2c436615672f402a9cacac79f27e1486",
      "2ae5a73a5dc6417ea9e7391190a5cc7e",
      "577489ab17364137ad0b6c5d7ee7ecd4",
      "4450371bafa84098bacf7ee6647432fc",
      "44715306855e4306b74b62422d46e391",
      "33c393307c7a4c009ea2f88e7b131e73",
      "327deedb88bb494e8ef10200e0236c6e",
      "58ee3bef989e4630bc631ce216ee020f",
      "c70449070691401b8048f32327a56583",
      "be7531a6c91b4961859b9d090e4af2a6",
      "34ff50e25d024d8da50ad71d9323ab51",
      "521f74b924c04f58b47b4d6d6cb5bcde",
      "8c3e46a9109f4682ad97c5dc7ec53c45",
      "57d2e3e0e6994e5aa752e42f5b90d34d",
      "c2a3efb6ff6644d0ae5e3ac2114755ac",
      "10ea4421b183443795ea483f73c3d280",
      "bf5fb1e5ad0f43b38b6bd966f80df889",
      "8eb9ae07195049e38c363e0f22c2d46c",
      "6439637aeb594e4fbb78886f49abd418",
      "cec31f90c4034a76bfc20e97110d7e6f",
      "4e02d2bc16394610bf8850a83b4570b7",
      "a8cfa49f0f9741569800248fab63bd81",
      "d937f21517884407a91fe0186ee5d19e",
      "a6b842b0e9f04931bd4322b2a9a180ea"
     ]
    },
    "id": "-InQzIboDs2j",
    "outputId": "98d6d0ea-d641-436c-ef48-021e39e0f8d2"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Environment ==\n",
      "{'python': '3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]', 'torch': '2.8.0+cu126', 'transformers': '4.51.3', 'accelerate': '1.4.0', 'peft': '0.14.0', 'cuda': '12.6', 'device_name': 'NVIDIA A100-SXM4-40GB', 'platform': 'Linux-6.6.105+-x86_64-with-glibc2.35', 'seed': 42}\n",
      "Loading GSM8K…\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "828b3bb4d2df4f5c8332bb9c44f561e0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c14c2eac7d434ffd896e33e3e9bfdc98"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8551488212854178b74385b209b47bf8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "474533c58b8242d898b2fac4d8869c52"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b79a82873e1548c9b251ab67e7c84de0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Splits: 7273 train | 200 val | 1319 test\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbe521d6446d4cea8cc7af8c702b28c2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdc72c477027405d850c978ef0fbeeaf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9cf70e54cb746bf9c49cfbc61dd5a1b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7e25a4b3caf4d1494a1931b2b8a4c4f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96faaf1fd485441ab9184063f5a3266b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24eca0174df04bd89f8db4b2712ffe95"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "577489ab17364137ad0b6c5d7ee7ecd4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57d2e3e0e6994e5aa752e42f5b90d34d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Padding sides: left left\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's establish baseline scores on the test set of GSM8K, so we know if we're able to improve the student or not.\n",
    "\n",
    "EM is Exact Match (accuracy score) on the val/test sets of GSM8K. Note that OPD doesn't use answer accuracy to inform weight updates during training! But it's still relevant for us to know since we do care about the accuracy."
   ],
   "metadata": {
    "id": "zd07L4MAVBaT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# --------------------------\n",
    "# Evaluation utils\n",
    "# --------------------------\n",
    "def _encode_cuda(tokenizer, texts: List[str], max_length=2048) -> Dict[str, torch.Tensor]:\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return {k: v.to(\"cuda\") for k, v in enc.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, tokenizer, dataset, *, num_examples: Optional[int] = None,\n",
    "             temperature: float = 0.0, max_new_tokens: int = 256,\n",
    "             batch_size: int = 32, desc: str = \"Eval\") -> float:\n",
    "    \"\"\"Exact-match accuracy against bracketed numeric answer.\"\"\"\n",
    "    n = len(dataset) if num_examples is None else min(num_examples, len(dataset))\n",
    "    rows = dataset.select(range(n))\n",
    "    correct = 0\n",
    "\n",
    "    was_cache = getattr(model.config, \"use_cache\", True)\n",
    "    model.eval(); model.config.use_cache = True\n",
    "\n",
    "    for i in tqdm(range(0, n, batch_size), desc=desc):\n",
    "        batch = rows.select(range(i, min(i + batch_size, n)))\n",
    "        prompts = [render_prompt(ex[\"question\"]) for ex in batch]\n",
    "        enc = _encode_cuda(tokenizer, prompts)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "\n",
    "        outs = model.generate(**enc, **gen_kwargs)\n",
    "        txts = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "        for ex, text in zip(batch, txts):\n",
    "            pred = parse_pred(text) or \"\"\n",
    "            gold = parse_gold(ex[\"answer\"]) or \"\"\n",
    "            correct += int(pred == gold)\n",
    "\n",
    "    model.config.use_cache = was_cache\n",
    "    return correct / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def preview(model, tokenizer, dataset, k=2, temperature=0.0, max_new_tokens=256):\n",
    "    model.eval(); model.config.use_cache = True\n",
    "    rows = dataset.select(range(min(k, len(dataset))))\n",
    "    for ex in rows:\n",
    "        prompt = render_prompt(ex[\"question\"])\n",
    "        enc = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        gen_kwargs = dict(max_new_tokens=max_new_tokens, use_cache=True,\n",
    "                          pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        out = model.generate(**enc, **gen_kwargs)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(\"=\"*80); print(prompt); print(\"-\"*80); print(text)\n",
    "        print(\"-\"*80, f\"\\nParsed: [{parse_pred(text)}] | Gold: [{parse_gold(ex['answer'])}]\")\n",
    "\n",
    "# --------------------------\n",
    "# Baselines (before training)\n",
    "# --------------------------\n",
    "print(\"\\n== Loading models for baseline evals ==\")\n",
    "student_for_eval = make_lora_student(cfg.student_id)\n",
    "teacher = load_teacher(cfg.teacher_id)\n",
    "\n",
    "print(\"\\nPreview (student, greedy)…\")\n",
    "preview(student_for_eval, tok_s, ds_test, k=1, temperature=0.0, max_new_tokens=cfg.max_new_tokens)\n",
    "\n",
    "print(\"\\nComputing baselines (greedy)…\")\n",
    "baseline_student_em = evaluate(student_for_eval, tok_s, ds_test,\n",
    "                               temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "                               batch_size=128, desc=\"Student baseline (test)\")\n",
    "baseline_teacher_em = evaluate(teacher, tok_t, ds_test,\n",
    "                               temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "                               batch_size=64, desc=\"Teacher baseline (test)\")\n",
    "print(f\"Student (0.6B base) EM: {baseline_student_em:.4f}\")\n",
    "print(f\"Teacher (4B instruct) EM: {baseline_teacher_em:.4f}\")\n",
    "\n",
    "with open(os.path.join(cfg.run_root, \"baselines.json\"), \"w\") as f:\n",
    "    json.dump(dict(student_0p6b=baseline_student_em, teacher_4b=baseline_teacher_em), f, indent=2)\n",
    "\n",
    "# Free the temporary student used just for baselines\n",
    "del student_for_eval; torch.cuda.empty_cache()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953,
     "referenced_widgets": [
      "72b8b514a48f4ca0a67b326aa3637771",
      "e565225c22bf46fd830c9aa347e1a6a8",
      "88e2bbeea7a64b1b9ef4814f1ba56265",
      "20685111bc904244953dd37e862a0b94",
      "41aefcb67cd44fcc9ea109e44c634bda",
      "d8fc382cd8b74a639812aa754d2e4892",
      "d00a60f623524613ab067c258792eac8",
      "b14e846853344ef889c9cf4793f6e920",
      "33595340276a44168e654b366eafd3e2",
      "46b1dfdf975c47ef89f6b8bd4d7bb9cd",
      "46c12b05d04148eaab60a9da1173320f",
      "dbc314e21cdf46a7a03cad16457ef48a",
      "8ba6277f82ab42e8a48056062ba9a81f",
      "d6742880c4d141758f69290990682eb3",
      "aa1352423f1e46e0852b708c8bfde7f0",
      "145a45b4adb940d2b3ea87093d2828fa",
      "638bf05ef3554b1c8127f615d11a960b",
      "abb4f1750ea3430cbf35f5676e3c6d35",
      "985cc598cb8a48eb819ae6f87750edc3",
      "969e13906b204dbaab11f69f9b8672d5",
      "58cf0d3d0d1b447ea3605b5843d027ad",
      "f15bd0bdb5fb4c6fa439958c49c12ac2",
      "747786162a9b40edb595a752e163bbfd",
      "84687a3ed5b64274a02ae6ee31ef0e92",
      "858397a66cb4412989c7c81f68d715fe",
      "b9d769bed0f04f2eae41c91b5886dcae",
      "61991f072c8a45469ad81bfea7c10647",
      "6c38ad827a144d41afc40af2ad007ea0",
      "3ade86a91d804158a0d41a082bfdbb83",
      "c41aa0ee9c274c2f89a0b82a519a8984",
      "92743ee5a2bc409f97dff157072b8546",
      "a24c682a4510488484ec35cc7051df93",
      "670c5d1517cb475bb1f56204e36c9300",
      "327d0e946ec147bcaa25192a3724c3ef",
      "4e73fac435dc4babb3b0acaa16dfafb3",
      "3b04bc0be8a7428d85e995eabf58ad98",
      "3a3c682814354662aafa372065e1f7f8",
      "2f066deac70a434a8acb99c7541ded25",
      "8047cf0699e44e918f2edafc8b2949ee",
      "aaac89c673d94a9fa6df18a155751855",
      "4fd3209f80a24a3984f69100c6d1864f",
      "0a2900ee0d1b4e5da83651185d765152",
      "2a1d50658f094af1b38365604d441d44",
      "bbf106cb1cd847688ae0ee7f84502fd8",
      "654d21c66ef6457fb3542127a2b17ce7",
      "f6b2f9280aa34e3a9af8707aebcf0a5c",
      "313fd5d1f9c3427a8e984c6bcf55c6e5",
      "1cbe460ebe00431fbd5d7613ef01629d",
      "e549e16be4114bc28a6215aeecbba54c",
      "4cb1667eaaac41bba521b66c9a24c6a2",
      "75dd6977fde24358a3aec66bccccf922",
      "face3670645e429dbd4a6167c879a3f0",
      "286bebca877741a49615f7a65e26b644",
      "1e9b0a89098d4f59b2980928283d48a2",
      "554e82c41435484886b57451a44d12ec",
      "6161225ac87746089d42eee6f777a67a",
      "10036385709b40adadacec695152b86e",
      "22232cb886e34489908a4afc1209a42f",
      "68bf56984ba14564892b4957ee757481",
      "9fffa449cc2442c6b7f064a0c62a041d",
      "9685b91cec144e0b9e6189fa95e8a311",
      "044e85a64b364e8eab394bf64156f338",
      "77d7e9b3d7514f51b11a7b2b6aeac163",
      "4a0489897a0c43fc909e338de2db2f9d",
      "0ea638797ea942a69e97d49396f302b7",
      "1f87721deb1e49709042ebe285dcce8f",
      "fcd2f948cfc24fd49081d22021f450ff",
      "05143d014b7046a8b71559661e926e26",
      "99eb264f634b47dfa8654ab09b75b839",
      "0227a53c5bc24896a980117c1ac5587b",
      "da1c266b08a24773ab566f5357ce35cf",
      "f7cf4525c1cf463b8f451d5fbe97b749",
      "ba5e3cdf1f86498eab4bac2d07731fd5",
      "73ea9df187134f9485c1705b4427e116",
      "97de14a9f89e4425b370e5a825725409",
      "f66277ce3e2a4e8e8f4f845152f27f0d",
      "806f7d0bf75343c298f16ba120d7fbc8",
      "932117d072dc4976952ddd44b4d857ae",
      "5a8f1773f228411aa55fbaef548957ae",
      "ff31ec78cae74944b0da5a0455e7d1e4",
      "154a9b4cf66a4ae7b172d93268519f82",
      "7540ac9cff284a0ea137d30995c74c64",
      "31fbe096629c4fe6a47340c686abd8dd",
      "8de604c484ee459fb95ec68179b29280",
      "3ebbe2e01b204f8da211e1c85a802b0c",
      "91270d620c4b4f8e95026aa02e840497",
      "2a040fb5c9b04219b6a9ed4ed09d6ba9",
      "064e6f1892c546a293d4a646e913dfd3",
      "f18684d5eecf469ab498336e9f36d23f",
      "965688ca7fde4175b478cba39aba6a8c",
      "084ae7dd0cef46c5a931b76e96b78005",
      "bbc71dab57ae47759c3fd4a5cfd7125e",
      "8f6a18534b7f4d338f7c9fcaeca2b550",
      "63c3b7b18c684ae58a51bb210303db10",
      "7eacbb6c19bc457d821d2f94cbf187cc",
      "e89544b654f144de974d7f8a07a6b790",
      "f534614af70b4e2494c75a752f364a6c",
      "3140d8341db7417a9c8a14b69a543c10",
      "397e6f5cc84844c4a632ec35c5c414a8",
      "00ff538a230e489c92346abec17a6446",
      "f6dbad2f239f411ba6a1b04919ef5e18",
      "a5c390a21731452c85c45c69cd58b0d6",
      "ce1c81f41cf24aa196509f683df889cf",
      "f47e7e34ec4d4d9ab9adef14550126d4",
      "e296d9dcdc43487290702abea4b93c0e",
      "1eef5a94b0794deda9e0a0b28d0653fb",
      "6382f544768a4104bdf76416c864e987",
      "1811f78246ee40b2894275ecc42f837b",
      "b66bc02dbb684e788c522de946bf2fa4",
      "959b656553fc4a64a28464466da0fcf0",
      "cb3b63f7bb364aaab0be66043a4d4fa5",
      "9d6107e082264679b0311586f2b7d524",
      "85212966823a48e4b602e7a8393e5764",
      "7fc214df1aa8443ca5cf91e4dbac821a",
      "3f1cc2d8643d4aec8a73e134dc2c360a",
      "5e2ea541e6ef4bdf92b22fb0f1372c11",
      "767bbfb01e834e8abb404aec39bfffde",
      "25c62b0f02ae43769b631d90b8ea472b",
      "fd98ef81a1cb4625b8a167e7312b6f8e",
      "2b014e1d79584bb0a0ea60b1936a45fa",
      "91ad82dc5db64c1fbe8b9e752f20f536",
      "8bd5ca463fdf45cfb43d788be1d7dbc6",
      "6150491f88694f39b2ff1af4b5de09fe",
      "9b56a1edf4674f82931c6a19ff857bf5",
      "d4bd222988f64eb4b83c06613659732f",
      "984a1714c781437bbd2af2adacdc38f8",
      "6b1bdb9f11134bfcb13d08b66a6962be",
      "a07e306e352c4ae59d427735a866b7ea",
      "67078c2932f7492b881442fd10924ca2",
      "1cda52825fcb4135ad271521db34e655",
      "dc002aabbe5542baafbaa3ee83b75d06",
      "ed66c998da374fae9329c0bb96ee2440",
      "66a0e41f1f864e9c98a73ed426b10419",
      "7fcc714096f14a6d9d560db4b6559d60",
      "0e9f018417ea46dcb6e1939a6f8e87c9",
      "84af1f504a24439ba7ff1d195fb128f4",
      "e14d945061f543d0a025b126dbacf4e6",
      "7aee6d179691471d820a9af73a0f6c54",
      "c95c2e47be504d2881b0001e32bfbc41",
      "399b2269ec9d486bb6b62a735cff7c59",
      "48bce59f7b154bc59a9ca0c7a0b1fd98",
      "0eabc211c3274f8990341133e05e0df5",
      "f5099c05c2744f629ec28998ad1da62c"
     ]
    },
    "id": "ToavEYPqVF6t",
    "outputId": "74a4d12c-002e-4921-e09e-7e6fc0cefc8e"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "== Loading models for baseline evals ==\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72b8b514a48f4ca0a67b326aa3637771"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbc314e21cdf46a7a03cad16457ef48a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "747786162a9b40edb595a752e163bbfd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "327d0e946ec147bcaa25192a3724c3ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "654d21c66ef6457fb3542127a2b17ce7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6161225ac87746089d42eee6f777a67a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcd2f948cfc24fd49081d22021f450ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "932117d072dc4976952ddd44b4d857ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f18684d5eecf469ab498336e9f36d23f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00ff538a230e489c92346abec17a6446"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb3b63f7bb364aaab0be66043a4d4fa5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Preview (student, greedy)…\n",
      "================================================================================\n",
      "Solve step by step.\n",
      "Give ONLY ONE final numeric answer (no units), inside square brackets.\n",
      "Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "Solution:\n",
      "--------------------------------------------------------------------------------\n",
      "Solve step by step.\n",
      "Give ONLY ONE final numeric answer (no units), inside square brackets.\n",
      "Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "Solution: 16 - 3 - 4 = 9 ducks are left to sell. 9 * 2 = $18.\n",
      "-------------------------------------------------------------------------------- \n",
      "Parsed: [18] | Gold: [18]\n",
      "\n",
      "Computing baselines (greedy)…\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Student baseline (test):   0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bd5ca463fdf45cfb43d788be1d7dbc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Teacher baseline (test):   0%|          | 0/21 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66a0e41f1f864e9c98a73ed426b10419"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Student (0.6B base) EM: 0.3867\n",
      "Teacher (4B instruct) EM: 0.8309\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's actually do the OPD training!"
   ],
   "metadata": {
    "id": "0gpLOq52TMRl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we train, we'll output the following metrics every 10 steps."
   ],
   "metadata": {
    "id": "pJ2g5hwKT24X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Column        | Meaning                                                                                                                                                                                                         | How to interpret                                                                      |\n",
    "| :------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |\n",
    "| **step**      | The current training iteration (out of the total configured `cfg.steps`, e.g. 100). Each step processes one batch of sampled prompts.                                                                           | Training progresses along this axis.                                                  |\n",
    "| **loss**      | The instantaneous batch loss (reverse-KL–style policy-gradient objective). Negative values are expected because we optimize `-E[(log p_T − log p_S) · log p_S]`; good updates drive this lower (more negative). | Lower (more negative) → better alignment with teacher on that batch.                  |\n",
    "| **loss_ema**  | Exponential moving average (EMA) of the loss across steps (with momentum = `cfg.ema_momentum`, e.g. 0.9). Smooths noise to show trend.                                                                          | Downward trend = overall improvement; steadiness = convergence.                       |\n",
    "| **revkl**     | The *reverse-KL divergence estimate* for this batch: roughly E[ log p_S − log p_T ]. It measures how far the student’s token distribution is from the teacher’s on its own rollouts.                            | Smaller (approaching 0) → student policy is closer to teacher.                        |\n",
    "| **revkl_ema** | EMA-smoothed version of reverse-KL, again for trend stability.                                                                                                                                                  | Should decrease steadily if distillation is working.                                  |\n",
    "| **tokens**    | Cumulative count of *valid graded tokens* seen so far — all pre-EOS generated tokens used for loss computation. It grows as training proceeds.                                                                  | Measures total learning signal processed; roughly proportional to compute/throughput. |\n",
    "| **val_em**    | Validation exact-match accuracy (fraction of GSM8K val examples where the parsed numeric answer matches gold). Evaluated every `cfg.val_every` steps on `cfg.val_sample_n` examples.                            | Direct measure of task performance; higher = better reasoning accuracy.               |\n"
   ],
   "metadata": {
    "id": "69mYi3WcTtnj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# --------------------------\n",
    "# OPD utilities\n",
    "# --------------------------\n",
    "def mask_before_first_eos(next_ids: torch.Tensor, eos_id: int) -> torch.Tensor:\n",
    "    \"\"\"Mask tokens strictly before the first EOS in each sequence.\"\"\"\n",
    "    is_eos = (next_ids == eos_id)\n",
    "    csum = is_eos.cumsum(dim=1)\n",
    "    return csum.eq(0)\n",
    "\n",
    "def student_logp_batched(student, pad_id, full_ids, next_ids, T, micro_bsz):\n",
    "    \"\"\"Return student log p(a_t) for the last T tokens (with grad).\"\"\"\n",
    "    outs = []\n",
    "    for s in range(0, full_ids.size(0), micro_bsz):\n",
    "        chunk = full_ids[s:s+micro_bsz]; nxt = next_ids[s:s+micro_bsz]\n",
    "        out = student(input_ids=chunk[:, :-1],\n",
    "                      attention_mask=(chunk[:, :-1] != pad_id))\n",
    "        logits = out.logits[:, -T:, :]\n",
    "        logp = F.log_softmax(logits, dim=-1).gather(-1, nxt.unsqueeze(-1)).squeeze(-1)\n",
    "        outs.append(logp)\n",
    "        del out, logits\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "# ---------- Cross-tokenizer teacher scoring ----------\n",
    "def _decode_token_str(tokenizer, token_id: int) -> str:\n",
    "    # Decode one token to text exactly as-is (keep spaces/prefixes).\n",
    "    return tokenizer.decode([int(token_id)],\n",
    "                            skip_special_tokens=False,\n",
    "                            clean_up_tokenization_spaces=False)\n",
    "\n",
    "def _encode_text_ids(tokenizer, text: str):\n",
    "    return tokenizer(text,\n",
    "                     add_special_tokens=False,\n",
    "                     return_tensors=\"pt\").input_ids[0].tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def teacher_logp_grouped_by_student_tokens(\n",
    "    teacher, tok_teacher, tok_student, prompts: List[str], next_ids: torch.Tensor, max_len: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    For each sample b and student step t:\n",
    "      1) Decode the student's token id next_ids[b,t] → text piece\n",
    "      2) Tokenize that text with the teacher tokenizer (may become multiple tokens)\n",
    "      3) Sum teacher log-probs over that group\n",
    "    Returns: Tensor [B, T] on CUDA with per-student-step teacher log-probs.\n",
    "    \"\"\"\n",
    "    device = teacher.device if hasattr(teacher, \"device\") else \"cuda\"\n",
    "    B, T = next_ids.shape\n",
    "    out = torch.zeros((B, T), device=device, dtype=torch.float32)\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = int(getattr(teacher.config, \"max_position_embeddings\", 2048))\n",
    "\n",
    "    for b in range(B):\n",
    "        prompt_text = prompts[b]\n",
    "        ctx_ids = _encode_text_ids(tok_teacher, prompt_text)\n",
    "\n",
    "        groups = []\n",
    "        for t in range(T):\n",
    "            s_tok_id = int(next_ids[b, t].item())\n",
    "            piece = _decode_token_str(tok_student, s_tok_id)\n",
    "            ids_t = _encode_text_ids(tok_teacher, piece)\n",
    "            groups.append(ids_t)\n",
    "\n",
    "        flat_gen = [tid for g in groups for tid in g]\n",
    "        if len(flat_gen) == 0:\n",
    "            continue\n",
    "\n",
    "        # Respect the teacher's context length by trimming left context\n",
    "        total = len(ctx_ids) + len(flat_gen)\n",
    "        if total > max_len:\n",
    "            overflow = total - max_len\n",
    "            ctx_ids = ctx_ids[overflow:]\n",
    "\n",
    "        # Build teacher-forcing inputs; labels are shifted by one token\n",
    "        full = ctx_ids + flat_gen\n",
    "        if len(full) < 2:\n",
    "            continue\n",
    "\n",
    "        input_ids = torch.tensor(full[:-1], device=device).unsqueeze(0)\n",
    "        labels    = torch.tensor(full[1:],  device=device).unsqueeze(0)\n",
    "        attn_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        outputs = teacher(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        logprobs = F.log_softmax(outputs.logits, dim=-1)\n",
    "        tok_lp = logprobs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)[0]  # [L]\n",
    "\n",
    "        # Labels index full[1:], so generated part starts at k = len(ctx_ids)-1\n",
    "        start = max(len(ctx_ids) - 1, 0)\n",
    "        gen_lp = tok_lp[start : start + len(flat_gen)]\n",
    "\n",
    "        # Sum back per student step\n",
    "        off = 0\n",
    "        for t, g in enumerate(groups):\n",
    "            k = len(g)\n",
    "            if k > 0:\n",
    "                out[b, t] = gen_lp[off : off + k].sum()\n",
    "            off += k\n",
    "\n",
    "    return out\n",
    "\n",
    "class LiveTable:\n",
    "    def __init__(self, title: str = \"Training metrics\", max_rows: int = 200):\n",
    "        self.title = title\n",
    "        self.max_rows = max_rows\n",
    "        self.rows = []\n",
    "        empty = pd.DataFrame(columns=[\"step\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle = display(self._styled(empty), display_id=True)\n",
    "\n",
    "    def _styled(self, df: pd.DataFrame):\n",
    "        styler = (\n",
    "            df.style\n",
    "              .set_caption(self.title)\n",
    "              .format({\n",
    "                  \"loss\": \"{:.4f}\",\n",
    "                  \"loss_ema\": \"{:.4f}\",\n",
    "                  \"revkl\": \"{:.4f}\",\n",
    "                  \"revkl_ema\": \"{:.4f}\",\n",
    "                  \"val_em\": (lambda v: \"\" if pd.isna(v) else f\"{v:.3f}\"),\n",
    "              })\n",
    "        )\n",
    "        try:\n",
    "            styler = styler.hide(axis=\"index\")\n",
    "            return styler\n",
    "        except Exception:\n",
    "            pass\n",
    "        return styler.set_table_styles([\n",
    "            {\"selector\": \"th.row_heading\", \"props\": [(\"display\", \"none\")]},\n",
    "            {\"selector\": \"th.blank\",       \"props\": [(\"display\", \"none\")]},\n",
    "        ])\n",
    "\n",
    "    def update(self, *, step, loss, loss_ema, revkl, revkl_ema, tokens, val_em=None):\n",
    "        self.rows.append(dict(\n",
    "            step=int(step),\n",
    "            loss=float(loss),\n",
    "            loss_ema=(None if loss_ema is None else float(loss_ema)),\n",
    "            revkl=float(revkl),\n",
    "            revkl_ema=(None if revkl_ema is None else float(revkl_ema)),\n",
    "            tokens=int(tokens),\n",
    "            val_em=(None if val_em is None else float(val_em)),\n",
    "        ))\n",
    "        rows = self.rows[-self.max_rows:]\n",
    "        df = pd.DataFrame(rows, columns=[\"step\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle.update(self._styled(df))\n",
    "\n",
    "# --------------------------\n",
    "# Training loop (verbose)\n",
    "# --------------------------\n",
    "def ema(prev, new, beta):\n",
    "    return new if prev is None else (beta * prev + (1 - beta) * new)\n",
    "\n",
    "def run_training(run_dir: str):\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Fresh student (LoRA adapters)\n",
    "    student = make_lora_student(cfg.student_id)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    prompts_all = [render_prompt(x[\"question\"]) for x in ds_train]\n",
    "    ema_loss = None\n",
    "    ema_revkl = None\n",
    "    tokens_graded_cum = 0\n",
    "    logs = []\n",
    "\n",
    "    table = LiveTable(title=\"On-Policy Distillation\")\n",
    "\n",
    "    pbar = tqdm(range(cfg.steps), desc=f\"OPD [{os.path.basename(run_dir)}]\")\n",
    "    for step in pbar:\n",
    "        # Deterministic batch selection per step\n",
    "        rng = np.random.default_rng(SEED + step)\n",
    "        idxs = rng.choice(len(prompts_all), size=cfg.batch_prompts, replace=False)\n",
    "        prompts = [prompts_all[i] for i in idxs]\n",
    "        prompts_rep = sum(([p] * cfg.samples_per_prompt for p in prompts), [])\n",
    "        enc = tok_s(prompts_rep, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 1) Student rollouts (no grad) to get sequences and step count\n",
    "        with torch.no_grad():\n",
    "            gen_out = student.generate(\n",
    "                **enc,\n",
    "                do_sample=True, temperature=cfg.train_temperature, top_p=0.9,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                eos_token_id=tok_s.eos_token_id, pad_token_id=tok_s.pad_token_id,\n",
    "                return_dict_in_generate=True, output_scores=True\n",
    "            )\n",
    "            seqs = gen_out.sequences\n",
    "            scores_list = list(gen_out.scores)  # per-step logits if you want to inspect\n",
    "            T = len(scores_list)\n",
    "            next_ids = seqs[:, -T:]\n",
    "            valid_mask = mask_before_first_eos(next_ids, eos_id=tok_s.eos_token_id).float()\n",
    "\n",
    "        # 2) Student log-probs with grad\n",
    "        student.train(); student.config.use_cache = False\n",
    "        logp_s = student_logp_batched(\n",
    "            student, tok_s.pad_token_id, seqs, next_ids, T, cfg.student_mb\n",
    "        )\n",
    "\n",
    "        # 3) Teacher log-probs (no grad), robust to tokenizer mismatches\n",
    "        teacher.eval()\n",
    "        logp_t = teacher_logp_grouped_by_student_tokens(\n",
    "            teacher=teacher,\n",
    "            tok_teacher=tok_t,\n",
    "            tok_student=tok_s,\n",
    "            prompts=prompts_rep,                  # the prompts used for these rollouts\n",
    "            next_ids=next_ids,                    # [B, T] student IDs\n",
    "            max_len=getattr(teacher.config, \"max_position_embeddings\", 2048),\n",
    "        )\n",
    "\n",
    "        # 4) Reverse-KL-style policy gradient\n",
    "        adv = (logp_t - logp_s).clamp(-5, 5)  # detach below for stability\n",
    "        denom = valid_mask.sum().clamp_min(1.0)\n",
    "        loss = - ((valid_mask * adv.detach()) * logp_s).sum() / denom\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rev_kl = ((logp_s - logp_t) * valid_mask).sum().item() / float(denom)\n",
    "            tokens_graded_cum += int(denom.item())\n",
    "            ema_loss = ema(ema_loss, float(loss.item()), cfg.ema_momentum)\n",
    "            ema_revkl = ema(ema_revkl, float(rev_kl), cfg.ema_momentum)\n",
    "\n",
    "        # Periodic validation EM (greedy)\n",
    "        val_em = None\n",
    "        if (step % cfg.val_every == 0) or (step == cfg.steps - 1):\n",
    "            student.eval(); student.config.use_cache = True\n",
    "            val_em = evaluate(student, tok_s, ds_val,\n",
    "                              num_examples=min(cfg.val_sample_n, len(ds_val)),\n",
    "                              temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                              batch_size=32, desc=\"VAL EM\")\n",
    "            student.train(); student.config.use_cache = False\n",
    "\n",
    "        row = dict(\n",
    "            step=int(step),\n",
    "            train_loss=float(loss.item()),\n",
    "            train_loss_ema=float(ema_loss) if ema_loss is not None else None,\n",
    "            train_revkl=float(rev_kl),\n",
    "            train_revkl_ema=float(ema_revkl) if ema_revkl is not None else None,\n",
    "            tokens_graded=int(tokens_graded_cum),\n",
    "            **({\"val_em\": float(val_em)} if val_em is not None else {})\n",
    "        )\n",
    "        logs.append(row)\n",
    "\n",
    "        if (step % cfg.log_every == 0) or (val_em is not None):\n",
    "            table.update(\n",
    "                step=row[\"step\"],\n",
    "                loss=row[\"train_loss\"],\n",
    "                loss_ema=row[\"train_loss_ema\"],\n",
    "                revkl=row[\"train_revkl\"],\n",
    "                revkl_ema=row[\"train_revkl_ema\"],\n",
    "                tokens=row[\"tokens_graded\"],\n",
    "                val_em=row.get(\"val_em\", None),\n",
    "            )\n",
    "\n",
    "        postfix = {\n",
    "            \"loss\": f\"{loss.item():.3f}\",\n",
    "            \"ema\": f\"{(ema_loss if ema_loss is not None else loss.item()):.3f}\",\n",
    "            \"rkl\": f\"{rev_kl:.3f}\",\n",
    "            \"toks\": tokens_graded_cum\n",
    "        }\n",
    "        if val_em is not None:\n",
    "            postfix[\"val\"] = f\"{val_em:.3f}\"\n",
    "        pbar.set_postfix(**postfix)\n",
    "\n",
    "        del scores_list\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save logs\n",
    "    try:\n",
    "        pd.DataFrame(logs).to_csv(os.path.join(run_dir, \"train_logs.csv\"), index=False)\n",
    "    except Exception:\n",
    "        with open(os.path.join(run_dir, \"train_logs.jsonl\"), \"w\") as f:\n",
    "            for r in logs:\n",
    "                f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    # Final test EM (greedy)\n",
    "    student.eval(); student.config.use_cache = True\n",
    "    test_em = evaluate(student, tok_s, ds_test,\n",
    "                       temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                       batch_size=64, desc=f\"Test EM [{os.path.basename(run_dir)}]\")\n",
    "\n",
    "    # Save adapters and summary\n",
    "    save_dir = os.path.join(run_dir, \"adapters_lora\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    student.save_pretrained(save_dir)\n",
    "\n",
    "    summary = dict(\n",
    "        steps=cfg.steps,\n",
    "        batch_prompts=cfg.batch_prompts,\n",
    "        samples_per_prompt=cfg.samples_per_prompt,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        train_tokens_graded=tokens_graded_cum,\n",
    "        test_em=float(test_em)\n",
    "    )\n",
    "    with open(os.path.join(run_dir, \"summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Free GPU\n",
    "    del student; torch.cuda.empty_cache()\n",
    "    return summary, logs\n",
    "\n",
    "# --------------------------\n",
    "# Run training (single run)\n",
    "# --------------------------\n",
    "print(\"\\n== Training (OPD) ==\")\n",
    "run_dir = os.path.join(cfg.run_root, \"opd\")\n",
    "summary, _ = run_training(run_dir)\n",
    "\n",
    "print(\"\\n== Final Results ==\")\n",
    "print(json.dumps(dict(\n",
    "    env=dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    ),\n",
    "    prompt_template=cfg.prompt_template,\n",
    "    baselines=dict(student_0p6b=baseline_student_em, teacher_4b=baseline_teacher_em),\n",
    "    final=summary,\n",
    "), indent=2))\n",
    "print(\"Artifacts saved to:\", cfg.run_root)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995,
     "referenced_widgets": [
      "365d83a1bc2140df97929c63f3c94d9f",
      "a3c10a6f5ec0479cbefa9bc2a01b8be7",
      "e330eaa07e644500955ae051ac8011b4",
      "53f75928a99a48b0b9af904588533326",
      "c8690b0e8e65438788062e181d94a2d6",
      "636e410e70d94503b4c8195a702bdb4e",
      "75ed02a3a6654802929b942d1dd28b2e",
      "56ce961cdb4c46bdb935bf010ad4875b",
      "246775adef3548cab461ec3586a528c3",
      "354256765ff140888bd5602070883643",
      "0f741692af69453a9e7caba4825965a2",
      "e4c887fc8d7a49d08b78de6f2af3f2c1",
      "7c6a69c08ef3499cb5090856fedbcf5d",
      "e3d70f0f33d84485aa322f3cdab3f4e0",
      "d935dfd3489b4621a5d10e3625574a0c",
      "65e5583b39964e1c90c9d9bdb5bb67eb",
      "a4ac88e6956f4774889c1cccd3c29297",
      "81df4caf536a43a39d0a718805b5fc7f",
      "7a31e3f5d3644e7c9bd71fc924fe51a6",
      "9ce1fc521a544627aaa0e85e779b0e23",
      "52132990d5b94deab10b468af8e938d7",
      "23305fc26a9743bd9315e6def42e5a8b",
      "a3fee3b38de949a288b2559a0b67d920",
      "eeb44e0193354d47b58baa48c89a2de7",
      "fa0a2ffaecbf44cf98750381ef6e5b1b",
      "a186ad8e50174791bc61aea8acc18ef0",
      "2efa2be3957d45d683f9eb817f12faff",
      "86d0274eea194b9ca5eae1483f7f7514",
      "a3579ceb44ba425cb0a29069f9344e44",
      "39739a50260a4c6f96def365cfa4b1ee",
      "8a14875674024b379327f9c863ff128a",
      "57c891e3b4034dc990f5414cd3259407",
      "c3c9698ad75a4d49a6efd41c673ae3f6",
      "83567dec2e284805b9afbf20b66b94cd",
      "80279bc7093a4a458c477f6966200dba",
      "b6fbc6349b284cf3ac60c76a396c1536",
      "6aa3e50a613043a5a4d579d9468a3b61",
      "7cf63ec8f716452eb1108b6923c22855",
      "dc386c60f668437ba1143cbfd8f4402b",
      "568d221533b24357945b4bd00e97f862",
      "4302a71ada0b439eab43875684f07db3",
      "3548b948f26e42ffa8ca2b92426ca0a9",
      "898c84f334ab456db5b6d52b32b8d791",
      "001824709b02447097282f556a1397ab",
      "7eddc91e84554878821a1c3a698ebf6c",
      "a8e76861c5214acd9d0798409e7206e6",
      "ffcd33497104439a957b92bee73a053e",
      "4761f9dc5bfd403ca170fcd2752c5761",
      "3e699380bf28412c866b0c1a2d4f5ef5",
      "d234220bb5cc48c58e423940c038155d",
      "a2637fbec04f4777aed0be9ff3119103",
      "2d175be415e449fc8b950748d97613b0",
      "b2d9c1a425be44a4ab6340546e4a3855",
      "52cb195e840244a7bbb1a8dfe8729f76",
      "ab810b6f5fab4576b3b84cd7a4d5ad3e",
      "6aa7ed15260049b683f8ee069873c390",
      "c38c1c86c0ca47408e43905cdda318cd",
      "8397c33566e942788d6ff99f9ff61f90",
      "66937d5026594b64800ed060a1f0c56c",
      "348b524d718e4669afc7330b3bcd1f5a",
      "b673baa42538418f9188033092da6b44",
      "86daea8296134a92aa34ec2ea175cb59",
      "5579fcd49f5c4889aa7cf682b4b55ca0",
      "b38f541499534f1f9a1795b16cbdaf27",
      "7c37b4081fdd4ed89d9b5947bd1db32c",
      "134310ca17944e46bb60582dc0a58831",
      "9827c4cff73a4005810390abb422ffd3",
      "20e6c487889c419bb75dc50a9ee4ea47",
      "5fddd8f0e8da4a03b098405450e989c4",
      "674b83673c424b518d1e18f0dc62f003",
      "002d79afffbb4eb98995b25bbe7e5bdc",
      "6d37d9b236a241b99d43e9d5212da62c",
      "b3d49b863ec94a27a1379bc46313b85f",
      "a0d54f5e6dff4a8dbed4dd3c789702e4",
      "2205e89386794d8aa569f579ee829d70",
      "71ef247ae3ff4b2691c29b63bec062e4",
      "91513f11a5d741e29258133b759a0737",
      "6f6c6e990cb947bb81fcd35b99381afe",
      "9796bdb43bd7448f87dd671f4bd148c3",
      "6e22bda2d4b3456c855ec9ed78754f98",
      "8566e90613004abd941063a56ee4a380",
      "1694eb0f3b894c4db18e5a562739046a",
      "b795df943a454d1f985ed47d5f9375a9",
      "af7a824b26404eef8f68c016760c3bae",
      "78c4efbf98c7433eac32d6aaf774429a",
      "0cf307e959274c56862cd371d84f945a",
      "63bd646e1ffe407c8705c4f8fa12beba",
      "7398c9cf9b464e0eade7794bf50f9f09"
     ]
    },
    "id": "tY7bzIZ6DwLc",
    "outputId": "acfe01bd-4c4e-4da0-96dc-d86062195dc3"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "== Training (OPD) ==\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7e4abc1b5f70>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4bd55\" class=\"dataframe\">\n",
       "  <caption>On-Policy Distillation</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_4bd55_level0_col0\" class=\"col_heading level0 col0\" >step</th>\n",
       "      <th id=\"T_4bd55_level0_col1\" class=\"col_heading level0 col1\" >loss</th>\n",
       "      <th id=\"T_4bd55_level0_col2\" class=\"col_heading level0 col2\" >loss_ema</th>\n",
       "      <th id=\"T_4bd55_level0_col3\" class=\"col_heading level0 col3\" >revkl</th>\n",
       "      <th id=\"T_4bd55_level0_col4\" class=\"col_heading level0 col4\" >revkl_ema</th>\n",
       "      <th id=\"T_4bd55_level0_col5\" class=\"col_heading level0 col5\" >tokens</th>\n",
       "      <th id=\"T_4bd55_level0_col6\" class=\"col_heading level0 col6\" >val_em</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_4bd55_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_4bd55_row0_col1\" class=\"data row0 col1\" >-0.3254</td>\n",
       "      <td id=\"T_4bd55_row0_col2\" class=\"data row0 col2\" >-0.3254</td>\n",
       "      <td id=\"T_4bd55_row0_col3\" class=\"data row0 col3\" >0.3572</td>\n",
       "      <td id=\"T_4bd55_row0_col4\" class=\"data row0 col4\" >0.3572</td>\n",
       "      <td id=\"T_4bd55_row0_col5\" class=\"data row0 col5\" >1748</td>\n",
       "      <td id=\"T_4bd55_row0_col6\" class=\"data row0 col6\" >0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4bd55_row1_col0\" class=\"data row1 col0\" >10</td>\n",
       "      <td id=\"T_4bd55_row1_col1\" class=\"data row1 col1\" >-0.2809</td>\n",
       "      <td id=\"T_4bd55_row1_col2\" class=\"data row1 col2\" >-0.2520</td>\n",
       "      <td id=\"T_4bd55_row1_col3\" class=\"data row1 col3\" >0.2494</td>\n",
       "      <td id=\"T_4bd55_row1_col4\" class=\"data row1 col4\" >0.2808</td>\n",
       "      <td id=\"T_4bd55_row1_col5\" class=\"data row1 col5\" >30496</td>\n",
       "      <td id=\"T_4bd55_row1_col6\" class=\"data row1 col6\" >0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4bd55_row2_col0\" class=\"data row2 col0\" >20</td>\n",
       "      <td id=\"T_4bd55_row2_col1\" class=\"data row2 col1\" >-0.4044</td>\n",
       "      <td id=\"T_4bd55_row2_col2\" class=\"data row2 col2\" >-0.2810</td>\n",
       "      <td id=\"T_4bd55_row2_col3\" class=\"data row2 col3\" >0.2459</td>\n",
       "      <td id=\"T_4bd55_row2_col4\" class=\"data row2 col4\" >0.2270</td>\n",
       "      <td id=\"T_4bd55_row2_col5\" class=\"data row2 col5\" >59299</td>\n",
       "      <td id=\"T_4bd55_row2_col6\" class=\"data row2 col6\" >0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4bd55_row3_col0\" class=\"data row3 col0\" >30</td>\n",
       "      <td id=\"T_4bd55_row3_col1\" class=\"data row3 col1\" >-0.3436</td>\n",
       "      <td id=\"T_4bd55_row3_col2\" class=\"data row3 col2\" >-0.3018</td>\n",
       "      <td id=\"T_4bd55_row3_col3\" class=\"data row3 col3\" >0.2464</td>\n",
       "      <td id=\"T_4bd55_row3_col4\" class=\"data row3 col4\" >0.2223</td>\n",
       "      <td id=\"T_4bd55_row3_col5\" class=\"data row3 col5\" >87205</td>\n",
       "      <td id=\"T_4bd55_row3_col6\" class=\"data row3 col6\" >0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4bd55_row4_col0\" class=\"data row4 col0\" >40</td>\n",
       "      <td id=\"T_4bd55_row4_col1\" class=\"data row4 col1\" >-0.3429</td>\n",
       "      <td id=\"T_4bd55_row4_col2\" class=\"data row4 col2\" >-0.2922</td>\n",
       "      <td id=\"T_4bd55_row4_col3\" class=\"data row4 col3\" >0.2059</td>\n",
       "      <td id=\"T_4bd55_row4_col4\" class=\"data row4 col4\" >0.1950</td>\n",
       "      <td id=\"T_4bd55_row4_col5\" class=\"data row4 col5\" >115343</td>\n",
       "      <td id=\"T_4bd55_row4_col6\" class=\"data row4 col6\" >0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4bd55_row5_col0\" class=\"data row5 col0\" >49</td>\n",
       "      <td id=\"T_4bd55_row5_col1\" class=\"data row5 col1\" >-0.2595</td>\n",
       "      <td id=\"T_4bd55_row5_col2\" class=\"data row5 col2\" >-0.2600</td>\n",
       "      <td id=\"T_4bd55_row5_col3\" class=\"data row5 col3\" >0.1220</td>\n",
       "      <td id=\"T_4bd55_row5_col4\" class=\"data row5 col4\" >0.1631</td>\n",
       "      <td id=\"T_4bd55_row5_col5\" class=\"data row5 col5\" >141744</td>\n",
       "      <td id=\"T_4bd55_row5_col6\" class=\"data row5 col6\" >0.730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "OPD [opd]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "365d83a1bc2140df97929c63f3c94d9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VAL EM:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4c887fc8d7a49d08b78de6f2af3f2c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VAL EM:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3fee3b38de949a288b2559a0b67d920"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VAL EM:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83567dec2e284805b9afbf20b66b94cd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VAL EM:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7eddc91e84554878821a1c3a698ebf6c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VAL EM:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6aa7ed15260049b683f8ee069873c390"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VAL EM:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9827c4cff73a4005810390abb422ffd3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Test EM [opd]:   0%|          | 0/21 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f6c6e990cb947bb81fcd35b99381afe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "== Final Results ==\n",
      "{\n",
      "  \"env\": {\n",
      "    \"python\": \"3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\",\n",
      "    \"torch\": \"2.8.0+cu126\",\n",
      "    \"cuda\": \"12.6\",\n",
      "    \"device\": \"NVIDIA A100-SXM4-40GB\"\n",
      "  },\n",
      "  \"prompt_template\": \"Solve step by step.\\nGive ONLY ONE final numeric answer (no units), inside square brackets.\\nProblem: {question}\\n\\nSolution:\",\n",
      "  \"baselines\": {\n",
      "    \"student_0p6b\": 0.3866565579984837,\n",
      "    \"teacher_4b\": 0.8309325246398787\n",
      "  },\n",
      "  \"final\": {\n",
      "    \"steps\": 50,\n",
      "    \"batch_prompts\": 4,\n",
      "    \"samples_per_prompt\": 4,\n",
      "    \"max_new_tokens\": 256,\n",
      "    \"train_tokens_graded\": 141744,\n",
      "    \"test_em\": 0.6186504927975739\n",
      "  }\n",
      "}\n",
      "Artifacts saved to: ./run_opd_1761781081\n"
     ]
    }
   ]
  }
 ]
}
